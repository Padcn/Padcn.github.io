---
title: Spark基础学习笔记
date: 2017-07-01
tags: Spark
---
# Spark基础学习笔记
> http://www.imooc.com/learn/814

Spark组件图：
![spark组件](http://img.mukewang.com/5957493700018e6419201080.jpg)

## Spark Core
包含基本功能：任务调度，内存管理，容错机制等。
内部定义了RDDs（弹性分布式数据库集）
提供了很多APIs来创建和操作这些RDDs。
作用：为其他组件提供底层的服务。

## SparkSQL
是Spark处理结构化数据的库，像Hive SQL,Mysql一样。
作用：企业中用来做报表统计。

## Spark Streaming
是实时数据流处理组件，类似Storm。
提供了API来操作实时流数据。
作用：企业中用来从Kafka或者其他消息队列接受数据做实时统计。

## Mlib
一个包含通用机器学习功能的功能，Machine learning lib。
包含分类，聚类，回归等，还包括模型评估，和数据导入。
<b>Mlib提供的这些方法，都支持集群上的横向扩展。</b>

机器学习

## Graphx
是处理图的库(例如，社交网络图)，并进行图的并行计算。
像Spark Streaming，Spark SQL一样，继承了RDD API。
提供了各种图的操作和常用图算法，例如PangeRank算法。

作用：图计算

## Cluster Managers
集群管理，Spark自带一个集群管理是单独调度器。
常见的集群管理包括Hadoop YARN，Apache Mesos

## 紧密集成：
 Spark底层优化了，基于Spark组件的组件也会得到优化。
 紧密集成，节省了各个组件组合使用时的部署，测试等时间。
 向Spark增加新的组件时，其他组件，可立即享用新组件的功能。
 
 ## 与Hadoop比较
 hadoop： 离线处理，对时效性要求不高。
 Spark：对时效性要求高（因为是基于内存的），机器学习等领域。
 
 Spark不具有HDFS的存储能力，要借助HDFS等持久化数据。
 
 ## Spark目录文件夹
 bin：包含Spark交互的可执行文件如Spark shell.
 core,streaming,python,...包含主要组件的源代码。
 examples包含单机Spark job，例子
 
 Spark的shell
 使得可以处理分布在集群上的数据。
 Spark巴蜀局加载到节点的内存中，因此分布式处理可在秒级完成。
 快速使迭代式计算，实时查询，分析一般在shells中完成。
 
 提供了Python Shell和Scala
 
 
 修改日志级别
 conf下的 log4j。properties。template
 log4j.rootCategory=WARN,console  更改级别可以控制日志的显示。
 
 ```scala
 val rdd=sc.parallelize(Array(1,2,3,4),4)
 rdd.count()
 rdd.foreach(print)//遍历打印内容 每次打印顺序随机因为，是分了4个片
 ```
 
 ## Scala基础
 变量申明 val或者var
 val 变量值不可修改，一旦分配不能重新指向别的值。
 var 分配后可以指向类型相同的值
 
 匿名函数和类型推断：
 lines.filter(line=>line.contains("world"))
 定义一个匿名函数，接受参数line
 使用line这个String类型变量的contains方法，返回结果
 line类型不需要指定，能够推断出来。
 
 ## Transformation
 	从之前的RDD构建一个新的RDD，像map()和filter()都是。
    
    逐元素：
    map()接受函数，把函数应用到RDD的每一个元素，返回新RDD。
    ```
 //使用数组生成lines的rdd
 val lines=sc.parallelize(Array("hell","ssps","hello","sss"));
 val lines2=lines.map(word=>(word,1))
    ```

filter接受一个函数，返回满足函数要求的
```
val liens3=lines.filter(word=>word.contains("hello"));
lines3.foreach(println);
```

flatMap()
对每个输入元素，输出多个输出元素。将RDD中元素压扁后返回一个新的RDD。
```
 val input=sc.textFIle("/home/xxx.txt");
 val lines=input.flatMap(line=>line.split(" "));
 lines.foreach(println)
```
 
 集合运算
 （并集交集）
 ```
  val rdd1=sc.paralelize(Array("coffe","coffe","panda","monkey","tea"))
  val rdd2=sc.paralelize(Array("coffe","kitty","monkey"))
 
 val rdd_distinct=rdd1.distinct() //去除重复。
 val rdd_union=rdd1.union(rdd2) //并集
 val rdd_inter=rdd1.intersection(rdd2)//交集
 val rdd_sub=rdd1.subtract(rdd2)//差集
 ```
 
 
 ## Action
 在RDD上计算出一个结果。
 把结果返回给driver program或者保存在文件系统上，比如：count()，save
 
 
 
 | 函数名 | 功能 | 例子 | 结果|
 |---|---|---|---|
  |collect()|返回RDD的所有元素|rdd.collect()|{1,2,3,3}|
  |count()|计数|rdd.count()|4|
  |countByValue()|返回一个map表示唯一元素出现的个数|rdd.countByValue()|{(1,1),(2,1),(3,2)}|
  |take(num)|返回几个元素|rdd.take(2)|{1,2}|
  |top(num)|返回前几个元素|rdd.top(2)|{3,3}|
  |takeOrdered(num)(ordering)|返回基于提供的排序算法的前几个元素|rdd.takeOrderd(2)(myOrdering)|{3,3}|
  |takeSample(withReplacement,num,[sedd])|取样例|rdd.takeSample(false,1)|不确定|
  |reduce(func)|合并RDD中元素|rdd.reduce((x,y)=>x+y)|9|
  |fold(zero)(func)|与reduce()相似提供zero value|rdd.fold(0)((x,y)=>x+y)|9|
  |aggregate(zeroValue)(seqOp,combOp)|与fold()相似,返回不同类型|rdd.aggregate((0,0))(x,y)=>(x._1+y,x._2+1),(x,y)=>(x._1+y._1,x_2+y._2)|(9,4)|
  |foreach(func)|对每个RDD的元素作用函数|rdd.foreach|无|
  
  常用：
  reduce():可以实现RDD中元素的累加，计数，和其他类型的聚集操作。
  ```scala
  val rdd=sc.parallelize(Array(1,2,3,3))
  rdd.collect()
  rdd.reduce((x,y)=>x+y) //得出结果9 1+2+3+3
  ```
  Collect()遍历整个RDD，向driver program返回RDD的内容。
  需要单机内存能够容纳下(因为数据要靠被给driver，一般情况下测试的时候使用)

数据大的时候一般使用saveAsTextFile() 

take(n)
返回RDD的n个元素（同时尝试访问最少的partitions）
返回的结果是无序的，测试的时候使用。

top()

```scala
rdd.top(1)
```

foreach()
计算RDD中每个元素，但不返回到本地。可以配合println()友好的打印数据。
  
 ## RDDs的特性
 ### 血统关系图
 Spark维护者RDDs之间的以来关系和创建关系，叫做血统关系图。
 Spark使用血统关系图来计算每个RDD的需求和恢复丢失的数据。
 
 ### 延迟计算（Lazy Evaluation）
 Spark对RDDs的计算是第一次使用action操作的时候。这种方式可以减少数据的传输，Spark内部记录metadata表名transformations操作已经被响应。
 
 数据加载也是延迟计算，数据只有在必要的时候才会被加载。
 
 ### RDD.persists()
 默认每次在RDDs上面进行action操作时，Spark都重新计算RDDs如果想要重复利用一个RDD，可以使用RDD.persist().
 unpersist()从缓存中移除。
 可以加入参数
 
 |级别|空间占用|cpu消耗|是否在内存中|是否在硬盘上|备注|
 |---|---|---|---|---|---|
 |MEMORY_ONLY|High|Low|Y|N||
 |MEMORY_ONLY_SER|Low|High|Y|N||
 |DISK_ONLY|Low|High|N|Y||
  |MEMORY_AND_DISK|High|Medium|Some|Some|内存放不下时，往硬盘上放|
   |MEMORY_AND_DISK_SER|Low|High|Some|Some|内存放不下时往硬盘上放，内存中数据时序列化|
 
 
 ## KeyValue对RDDs
 创建：
 map函数
 ```
 val rdd=sc.textFile("/home/xxx.txt");
 //以第一个元素作为key，整行为value
 val rdd2=rdd.map(line=>(line.split(" ")(0),line))
 ```
 
 常见操作
 
 |函数名|作用|例子|结果|
 |---|---|---|---|
 |reduceByKey(func)|把相同key结合|rdd.reduceByKey(x,y)=>x+y|{(1,2),(3,10)}|
 |groupByKey()|把相同key的values分组|rdd.groupByKey()|{(1,[2]),(3,[4,6])}|
  |combineByKey(createCOmbiner,mergeValue,mergeCombiners,partitioner)|把相同key的结合，使用不同的返回类型|||
sc.parallelize(Array((1,2),(3,4),(3,6))) 
 
 ```
  val rdd3=sc.parallelize(Array((1,2),(3,4),(3,6)))
  val rdd4=rdd3.reduceByKey((x,y)=>x+y)//按key相同则相加结果为（1,2）（3,10）
  val rdd5=rdd3.groupByKey()//根据key分组结果为(1,[2]),(3,[4,6])
 ```
 
 |函数名|作用|例子|结果|
 |---|---|---|---|
 |mapValues(func)|函数作用于pairRDD的每个元素，key不变|rdd.mapValues(x=>x+1)|{(1,3),(3,5),(3,7)}|
 |flatMapValues(func)|符号化的时候使用|rdd.flatMapValues(x=>(xto 5))|{(1,2),(1,3),(1,4),(1,5),(3,4),(3,5)}|
 |keys|仅返回keys|rdd.keys|{1,3,3}|
 |values|仅返回value|rdd.values|{2,4,6}|
 |sortByKey()|按照key排序|rdd.sortByKey()|{(1,2),(3,4),(3,6)}|
 
 
 ## combineByKey():
 参数列表（createCombiner,mergeValue,mergeCombiners,partitioner）
 最常用的基于key的聚合函数，返回类型可以与输入类型不一样。
 许多基于key的聚合函数都用到了它，如groupByKey().
 
 遍历partition中元素，元素的key，要么见过，要么不是。
 新元素使用createCombiner()函数，
 如果是已经存在的key则使用mergeValue()函数
 合计每个partition的结果时，使用mergeCOmbiners()函数
 
 ```scala
 //求平均值
 val scores=sc.parallelize(Array(("na",111),("na",101),("na",110),("ml",81),("ml",97),("ml",90)))
 //score代表值分数
 val score2=scores.combineByKey(
 score=>(1,score),(c1:(Int,Double),
 newScore)=>(c1._1+1,c1._2+newScore),
 (c1:(Int,Double),c2:(Int,Double)=>(c1._1+c2._1,c1._2+c2._2)))
 
 val average=score2.map{case(name,(num,score))=>(name,score/num)}
 ```
 > http://www.cnblogs.com/rigid/p/5563205.html
 
 
 










